---
title: 'Final project 527'
author: 'Tom Kang & Will Li'
date: "5/7/2022"
output:
  pdf_document: default
  html_document:
    keep_md: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Load Data
```{r}
raw_aba = read.csv("abalone.csv")
colnames(raw_aba) = c("Sex","Length","Diameter","Height","Whole_weight","Shucked_weight","Viscera_weight","Shell_weight","Rings")
head(raw_aba)
```



## EDA
```{r}
table(raw_aba$Rings)
```

```{r}
hist(raw_aba$Rings,breaks=29,main="Rings",xlab="rings",ylab="Count")
summary(raw_aba)
```



## Data Preprocessing

### Data Cleaning
```{r}
aba_1 = raw_aba[-c(which(raw_aba$Height == 0)),]
```

### Data Normalization
```{r}
norm <- function(x) {(x - min(x))/(max(x)-min(x))}
```

```{r}
aba_2 = aba_1

for (i in 2:8){
  aba_2[,i] = norm(aba_1[,i])
}
```

### One-Hot Encoding
```{r}
library(dummies)
aba_3 <- dummy.data.frame(aba_2, names=c("Sex"), sep="_")
```

### Outlier Detection
```{r}
library(OutlierDetection)
library(OutliersO3)
library(outliers)
```

```{r}
X = aba_1[2:8]
OD = OutlierDetection(X,depth = TRUE,dense = TRUE, distance = TRUE, dispersion = TRUE)
```

```{r}
Z = scores(X,type="z",prob=0.95)
MAD = scores(X,type="mad",prob=0.95)
IQR = scores(X,type="iqr",lim=1.5)
```

```{r}
Z_out = which(Z == TRUE)
MAD_out = which(MAD == TRUE)
IQR_out = which(IQR == TRUE)

out_1 = intersect(Z_out,MAD_out)
out_2 = intersect(out_1,IQR_out)
out = intersect(out_2,OD$`Location of Outlier`)
```

```{r}
aba = aba_3[-c(out),]
```

### Final Ready-to-use Data
```{r}
head(aba)
summary(aba)
```

### Train-Test Split
```{r}
set.seed(527)
train = sample(1:nrow(aba), nrow(aba)*0.7)
aba.train = aba[train,]
aba.test = aba[-train,]
aba.train.x = aba.train[,-c(11)]
original.train.y = aba.train[,c(11)]
aba.train[,c(11)] = norm(aba.train[,c(11)])
aba.train.y = aba.train[,c(11)]
aba.test.x = aba.test[,-c(11)]
aba.test.y = aba.test[,c(11)]
```

```{r}
revert <- function(x) {(x*(max(original.train.y)-min(original.train.y)))+min(original.train.y)}
```

## Simple Linear Regression
```{r}
baselm = lm(Rings~.-Sex_M,data = aba.train)
summary(baselm)
```

```{r}
plot(baselm)
```
```{r}
## Define a function that outputs train_mse, test_mse, and accuracy
summ <- function(model) {
  prediction = revert(predict(model,aba.test.x))
  print(paste("train_mse: ",mean((revert(predict(model,aba.train.x)) - revert(aba.train.y))^2)))
  print(paste("test_mse: ",mean((prediction - aba.test.y)^2)))
  print(paste("test_mae: ",mean(abs(prediction - aba.test.y))))
}
```

```{r}
summ(baselm)
```

```{r}
table(round(revert(predict(baselm,aba.test.x)))-aba.test.y)
```

```{r}
aba.train.lm = aba.train[-c(2305,480,294,2051),]
aba.train.x.lm = aba.train.x[-c(2305,480,294,2051),]
aba.train.y.lm = aba.train.y[-c(2305,480,294,2051)]
```

```{r}
baselm_1 = lm(Rings~.-Sex_M-Length-Sex_F,data = aba.train.lm)
summary(baselm_1)
summ(baselm_1)
```



## Feature SELECTION
```{r}
library(leaps)
```

```{r}
bss.summ = summary(regsubsets(Rings~.-Sex_M,data = aba.train.lm,nvmax = 9))
bss.summ
```

```{r}
plot(bss.summ$adjr2,xlab="Predictors", ylab = "Adjusted R^2", type = "l")
plot(bss.summ$cp,xlab="Predictors", ylab = "Cp", type = "l")
plot(bss.summ$bic,xlab="Predictors", ylab = "BIC", type = "l")
```



## Polynomial Feature Mapping
```{r}
polylm = lm(Rings~.+I(`Shell_weight`^5)+I(`Shucked_weight`^3)+I(`Diameter`^3)-Sex_M-Length-Sex_F,data = aba.train.lm)
summary(polylm)
summ(polylm)
```




## RF
```{r}
library(randomForest)
```


```{r}
rf <- randomForest(Rings~.,data=aba.train, mtry=5,ntree=100,importance=TRUE)
prediction = revert(predict(rf,newdata=aba.train))
mean((prediction-aba.train$Rings)^2)
mean(abs(prediction-aba.train$Rings))

prediction.test = revert(predict(rf,newdata=aba.test.x))
mean((prediction.test-aba.test.y)^2)
mean(abs(prediction.test-aba.test.y))
```


```{r}
nt=c()
nt.var = c()
nt.error = c()
for (i in seq(1,100,5)){
  rf.cv = rfcv(aba.train.x ,aba.train$Rings,cv.fold = 10, mtry=function(p) max(1, floor(sqrt(p))), ntree = i)
  nt = c(nt,i)
  nt.var = c(nt.var,rf.cv$n.var[which.min(rf.cv$error.cv)[[1]]])
  nt.error = c(nt.error,min(rf.cv$error.cv))
} 

```

```{r}
nt.var[which.min(nt.error)]
nt[which.min(nt.error)]
```

### Final Model
```{r}
rf.best = randomForest(Rings~.,data=aba.train, mtry=nt.var[which.min(nt.error)],ntree=nt[which.min(nt.error)],importance=TRUE)
prediction.best = revert(predict(rf.best,newdata=aba.train))
mean((prediction.best-aba.train$Rings)^2)
mean(abs(prediction.best-aba.train$Rings))

prediction.test.best = revert(predict(rf.best,newdata=aba.test.x))
mean((prediction.test.best-aba.test.y)^2)
mean(abs(prediction.test.best-aba.test.y))
```

```{r}
rf.cv$n.var[which.min(rf.cv$error.cv)[[1]]]
min(rf.cv$error.cv)
table(round(prediction.test.best-aba.test.y))
```



## ANN
```{r}
library(neuralnet)
```
### Cross Validation 
#### NOTE: IT TAKES ABOUT 1-1.5 HOURS TO FINISH THE CROSS VALIDATION PROCESS 
```{r}
grid = list(c(3),c(5),c(7),c(10),c(3,3),c(5,5),c(10,10),c(3,3,3),c(5,5,5),c(10,10,10))
cv.mse = c()

for (nodes in grid){
  mse = c()
  for (k in 1:5){
    set = (round((k-1)*nrow(aba.train)/5)+1):round(k*nrow(aba.train)/5)
    cv.test = aba.train[set,]
    cv.train = aba.train[-set,]
    
    cv.model = neuralnet(Rings~Sex_I+Sex_F+Diameter+Height+Whole_weight+Shucked_weight+Viscera_weight+Shell_weight,data=cv.train,linear.output = F, hidden = nodes)
    k.mse = mean((predict(cv.model,cv.test[,-c(11)])-cv.test[,c(11)])^2)
    mse = c(mse,k.mse)
  }
  cv.mse = c(cv.mse,mean(mse))
}
```

### Final Model
```{r}
ann <- neuralnet(Rings~Sex_I+Sex_F+Diameter+Height+Whole_weight+Shucked_weight+Viscera_weight+Shell_weight,data=aba.train,linear.output = F, hidden = grid[[which.min(cv.mse)]])
```

```{r}
plot(ann)
```
```{r}
print(paste("test MSE: ",mean((revert(predict(ann,aba.test.x))-aba.test.y)^2)))
print(paste("test MAE: ",mean(abs(revert(predict(ann,aba.test.x))-aba.test.y))))
```

```{r}
table(round(revert(predict(ann,aba.test.x)))-aba.test.y)
```











